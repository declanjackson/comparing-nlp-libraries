{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Comparing NLP Techniques\n",
    "\n",
    "This project sets out to compare different applications of NLP techniques through the following libraries:\n",
    "\n",
    "- Tensorflow\n",
    "- PyTorch\n",
    "\n",
    "This notebook will focus on `pytorch`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from torchtext.legacy import data\n",
    "import gzip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre Processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# lets open the data\n",
    "# sourced from https://registry.opendata.aws/fast-ai-nlp/\n",
    "\n",
    "column_names = [\"CATEGORY\", \"TITLE\", \"CONTENT\"]\n",
    "# we use the train.csv only\n",
    "news_df = pd.read_csv(\"data/train.csv\", names=column_names, header=None, delimiter=\",\")\n",
    "\n",
    "# make the category classes more readable\n",
    "mapping = {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}\n",
    "news_df = news_df.replace({'CATEGORY': mapping})\n",
    "news_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CATEGORY                                              TITLE  \\\n",
       "0  Business  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1  Business  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2  Business    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3  Business  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4  Business  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                             CONTENT  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# lets check the spread of labels\n",
    "news_df[\"CATEGORY\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Business    30000\n",
       "Sci/Tech    30000\n",
       "World       30000\n",
       "Sports      30000\n",
       "Name: CATEGORY, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# lets encode the target variable\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoded_y = encoder.fit_transform(news_df[\"CATEGORY\"].values)\n",
    "num_classes = len(encoder.classes_)\n",
    "\n",
    "# one hot encode integers\n",
    "dummy_y = np.eye(num_classes, dtype=\"float32\")[encoded_y]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# now we want to tokenize the title inputs\n",
    "MAX_VECTOR_LEN = 40\n",
    "\n",
    "docs = news_df[\"TITLE\"].values\n",
    "\n",
    "t = data.Field(\n",
    "    lower       = True,\n",
    "    tokenize   = \"basic_english\",\n",
    "    fix_length  = MAX_VECTOR_LEN\n",
    ")\n",
    "\n",
    "docs = list(map(t.preprocess, docs))\n",
    "padded_docs = t.pad(docs)\n",
    "t.build_vocab(padded_docs)\n",
    "print(f\"Vocabulary size: {len(t.vocab)}\")\n",
    "\n",
    "numericalized_docs = []\n",
    "for d in padded_docs:\n",
    "    temp = []\n",
    "    for c in d:\n",
    "        temp.append(t.vocab.stoi[c])\n",
    "    numericalized_docs.append(temp)\n",
    "\n",
    "print(f\"Number of headlines: {len(numericalized_docs)}\")\n",
    "processed_titles =  np.array(numericalized_docs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 39580\n",
      "Number of headlines: 120000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#  now we need to create the embedding matrix with only relevant embeddings\n",
    "\n",
    "embeddings_index = dict()\n",
    "with gzip.open('./data/cc.en.300.vec.gz', \"rt\") as zipf:\n",
    "    firstline = zipf.readline()\n",
    "    emb_vocab_size, emb_d = firstline.split(\" \")\n",
    "    emb_vocab_size = int(emb_vocab_size)\n",
    "    emb_d = int(emb_d)\n",
    "    for line in zipf:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        # Only load subset of the embeddings recognised by the tokenizer:\n",
    "        if word in t.vocab.stoi:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "print(\"Loaded {} of {} word vectors for tokenizer vocabulary length {}\".format(\n",
    "    len(embeddings_index),\n",
    "    emb_vocab_size,\n",
    "    len(t.vocab),\n",
    "))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 31687 of 2000000 word vectors for tokenizer vocabulary length 39580\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((len(t.vocab), emb_d))\n",
    "for word, i in t.vocab.stoi.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "vocab_size=embedding_matrix.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    processed_titles,\n",
    "    dummy_y,\n",
    "    test_size=0.2,\n",
    "    random_state=1\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PyTorch Implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# reset the seed\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# define and intialize the neural network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=400000, emb_dim=300, num_classes=4):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.conv1 = nn.Conv1d(emb_dim, 128, kernel_size=3)\n",
    "        self.max_pool1d = nn.MaxPool1d(5)\n",
    "        self.flatten1 = nn.Flatten()\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.fc1 = nn.Linear(896, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    # x is data that will be passed through the network\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = torch.transpose(x,1,2)\n",
    "        x = self.flatten1(self.max_pool1d(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# define helper function to train our model\n",
    "\n",
    "def train(train_loader, embedding_matrix, num_classes=4, epochs=12, learning_rate=0.001):\n",
    "\n",
    "    # initialise model\n",
    "    model = Net(\n",
    "        vocab_size=embedding_matrix.shape[0],\n",
    "        emb_dim=embedding_matrix.shape[1],\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    model.embedding.weight = torch.nn.parameter.Parameter(torch.FloatTensor(embedding_matrix), False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "        for batch_idx, (X_train, y_train) in enumerate(train_loader, 1):\n",
    "            data, target = X_train.to(device), y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.binary_cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        print(f\"epoch: {epoch}, train_loss: {running_loss / n_batches:.6f}\")  # (Avg over batches)\n",
    "    return model, device"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# define helper function to test our model\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.binary_cross_entropy(output, target, reduction=\"sum\").item()\n",
    "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            target_index = target.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target_index).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Average loss over dataset samples\n",
    "    print(f\"val_loss: {test_loss:.4f}, val_acc: {correct/len(test_loader.dataset):.4f}\") "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        X = torch.as_tensor(self.data[index]).long()\n",
    "        y = torch.as_tensor(self.labels[index])\n",
    "        return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "%%time\n",
    "# fit the model:\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "trainloader = DataLoader(Dataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "testloader = DataLoader(Dataset(X_test, y_test), batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Training model...\")\n",
    "model, device = train(\n",
    "    trainloader,\n",
    "    embedding_matrix,\n",
    "    num_classes=num_classes,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    ")\n",
    "print(\"Evaluating model...\")\n",
    "test(model, testloader, device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training model...\n",
      "epoch: 1, train_loss: 0.237832\n",
      "epoch: 2, train_loss: 0.201736\n",
      "epoch: 3, train_loss: 0.186768\n",
      "epoch: 4, train_loss: 0.176881\n",
      "epoch: 5, train_loss: 0.168310\n",
      "Evaluating model...\n",
      "val_loss: 0.7473, val_acc: 0.8566\n",
      "CPU times: user 3min 45s, sys: 15.5 s, total: 4min\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from IPython import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def classify(text):\n",
    "    \"\"\"Classify a headline and print the results\"\"\"\n",
    "    processed = t.preprocess(text)\n",
    "    padded = t.pad([processed])\n",
    "    final_text = []\n",
    "    for w in padded[0]:\n",
    "        final_text.append(t.vocab.stoi[w])\n",
    "    final_text = torch.tensor([final_text])\n",
    "    model.cpu()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = model(final_text)\n",
    "    print(result)\n",
    "    ix = np.argmax(result.detach())\n",
    "    print(f\"Predicted class: '{encoder.classes_[ix]}' with confidence {result[0][ix]:.2%}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "interaction = widgets.interact_manual(\n",
    "   classify,\n",
    "   text=widgets.Text(\n",
    "       value=\"The markets were bullish after news of the merger\",\n",
    "       placeholder=\"Type a news headline...\",\n",
    "       description=\"Headline:\",\n",
    "       layout=widgets.Layout(width=\"99%\"),\n",
    "   )\n",
    ")\n",
    "interaction.widget.children[1].description = \"Classify!\""
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ad8a555d440473697e3bcbbff764018"
      },
      "text/plain": [
       "interactive(children=(Text(value='The markets were bullish after news of the merger', description='Headline:',â€¦"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "8e93762ce26157543dddba731cb4c23b4c88be6f22dc743c1c6a9259770c1b2a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}