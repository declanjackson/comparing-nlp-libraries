{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Comparing NLP Techniques\n",
    "\n",
    "This project sets out to compare different applications of NLP techniques through the following libraries:\n",
    "\n",
    "- Tensorflow\n",
    "- PyTorch\n",
    "\n",
    "This notebook will focus on `tensorflow`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import gzip\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "\n",
    "READING_IN_EMBEDDINGS = False\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# lets open the data\n",
    "# sourced from https://registry.opendata.aws/fast-ai-nlp/\n",
    "\n",
    "column_names = [\"CATEGORY\", \"TITLE\", \"CONTENT\"]\n",
    "# we use the train.csv only\n",
    "news_df = pd.read_csv(\"data/train.csv\", names=column_names, header=None, delimiter=\",\")\n",
    "\n",
    "# make the category classes more readable\n",
    "mapping = {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}\n",
    "news_df = news_df.replace({'CATEGORY': mapping})\n",
    "news_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CATEGORY                                              TITLE  \\\n",
       "0  Business  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1  Business  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2  Business    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3  Business  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4  Business  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                             CONTENT  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# lets check the spread of labels\n",
    "news_df[\"CATEGORY\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Business    30000\n",
       "Sports      30000\n",
       "Sci/Tech    30000\n",
       "World       30000\n",
       "Name: CATEGORY, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# lets encode the target variable\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoded_y = encoder.fit_transform(news_df[\"CATEGORY\"].values)\n",
    "num_classes = len(encoder.classes_)\n",
    "\n",
    "# one hot encode integers\n",
    "dummy_y = np.eye(num_classes, dtype=\"float32\")[encoded_y]\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "MAX_VECTOR_LEN = 40\n",
    "\n",
    "# now we want to tokenize the title inputs\n",
    "titles = news_df[\"TITLE\"].values\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(titles)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_titles = t.texts_to_sequences(titles)\n",
    "\n",
    "# print the results\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# make every sequence the same length\n",
    "print(f\"Padding titles to max_length={MAX_VECTOR_LEN} \\\n",
    "        (truncating {sum(1 for doc in encoded_titles if len(doc) > MAX_VECTOR_LEN)} titles)\")\n",
    "padded_docs = pad_sequences(encoded_titles, maxlen=MAX_VECTOR_LEN, padding=\"post\")\n",
    "print(f\"Number of headlines: {len(padded_docs)}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 36758\n",
      "Padding titles to max_length=40         (truncating 0 titles)\n",
      "Number of headlines: 120000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Tokenizer.num_words is nullable, and there's an OOV token, so:\n",
    "tokenizer_vocab_size = len(t.word_index) + 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# we need to represent words as numeric values, so lets use pre trained word embeddings from https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "if not READING_IN_EMBEDDINGS:\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('./data/cc.en.300.vec.gz', 'wb').write(r.content)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#  now we need to create the embedding matrix with only relevant embeddings\n",
    "\n",
    "embeddings_index = dict()\n",
    "with gzip.open('./data/cc.en.300.vec.gz', \"rt\") as zipf:\n",
    "    firstline = zipf.readline()\n",
    "    emb_vocab_size, emb_d = firstline.split(\" \")\n",
    "    emb_vocab_size = int(emb_vocab_size)\n",
    "    emb_d = int(emb_d)\n",
    "    for line in zipf:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        # Only load subset of the embeddings recognised by the tokenizer:\n",
    "        if word in t.word_index:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "print(\"Loaded {} of {} word vectors for tokenizer vocabulary length {}\".format(\n",
    "    len(embeddings_index),\n",
    "    emb_vocab_size,\n",
    "    tokenizer_vocab_size,\n",
    "))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 29240 of 2000000 word vectors for tokenizer vocabulary length 36758\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# now we can delete the big .gz file\n",
    "if not READING_IN_EMBEDDINGS:\n",
    "    os.remove('./data/cc.en.300.vec.gz')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# create a weight matrix for words in training docs\n",
    "\n",
    "embedding_matrix = np.zeros((tokenizer_vocab_size, emb_d))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# save our embeddings for later\n",
    "\n",
    "np.save(\n",
    "    file=\"./data/docs-embedding-matrix\",\n",
    "    arr=embedding_matrix,\n",
    "    allow_pickle=False,\n",
    ")\n",
    "vocab_size=embedding_matrix.shape[0]\n",
    "print(embedding_matrix.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(36758, 300)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# spliting up our data for training and evaluating\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_docs,\n",
    "    dummy_y,\n",
    "    test_size=0.2,\n",
    "    random_state=1\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# save the docs\n",
    "\n",
    "np.save(\"./data/train_X.npy\", X_train)\n",
    "np.save(\"./data/train_Y.npy\", y_train)\n",
    "np.save(\"./data/test_X.npy\", X_test)\n",
    "np.save(\"./data/test_Y.npy\", y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensorflow Implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Embedding, Flatten, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "from IPython import display\n",
    "import ipywidgets as widgets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    embedding_matrix.shape[0],  # Final vocabulary size\n",
    "    embedding_matrix.shape[1],  # Word vector dimensions\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=40,\n",
    "    trainable=False,\n",
    "    name=\"embed\"\n",
    "))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation=\"relu\", name=\"conv_1\"))\n",
    "model.add(MaxPooling1D(pool_size=5, name=\"maxpool_1\"))\n",
    "model.add(Flatten(name=\"flat_1\"))\n",
    "model.add(Dropout(0.3, name=\"dropout_1\"))\n",
    "model.add(Dense(128, activation=\"relu\", name=\"dense_1\"))\n",
    "model.add(Dense(num_classes, activation=\"softmax\", name=\"out_1\"))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, 40, 300)           11027400  \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv1D)              (None, 38, 128)           115328    \n",
      "_________________________________________________________________\n",
      "maxpool_1 (MaxPooling1D)     (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flat_1 (Flatten)             (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               114816    \n",
      "_________________________________________________________________\n",
      "out_1 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 11,258,060\n",
      "Trainable params: 230,660\n",
      "Non-trainable params: 11,027,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# fit the model here in the notebook:\n",
    "print(\"Training model\")\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=5, verbose=1)\n",
    "print(\"Evaluating model\")\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(\n",
    "    \"Validation results: \"\n",
    "    + \"; \".join(map(\n",
    "        lambda i: f\"{model.metrics_names[i]}={scores[i]:.5f}\", range(len(model.metrics_names))\n",
    "    ))\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training model\n",
      "Epoch 1/5\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.2482 - acc: 0.8013\n",
      "Epoch 2/5\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.2087 - acc: 0.8405\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 22s 4ms/step - loss: 0.1991 - acc: 0.8509\n",
      "Epoch 4/5\n",
      "6000/6000 [==============================] - 22s 4ms/step - loss: 0.1948 - acc: 0.8583\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.1947 - acc: 0.8614\n",
      "Evaluating model\n",
      "750/750 - 2s - loss: 0.1999 - acc: 0.8573\n",
      "Validation results: loss=0.19989; acc=0.85729\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def classify(text):\n",
    "    \"\"\"Classify a headline and print the results\"\"\"\n",
    "    encoded_example = t.texts_to_sequences([text])\n",
    "    # Pad documents to a max length of 40 words\n",
    "    max_length = 40\n",
    "    padded_example = pad_sequences(encoded_example, maxlen=max_length, padding=\"post\")\n",
    "    result = model.predict(padded_example)\n",
    "    print(result)\n",
    "    ix = np.argmax(result)\n",
    "    print(f\"Predicted class: '{encoder.classes_[ix]}' with confidence {result[0][ix]:.2%}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "interaction = widgets.interact_manual(\n",
    "    classify,\n",
    "    text=widgets.Text(\n",
    "        value=\"The markets were bullish after news of the merger\",\n",
    "        placeholder=\"Type a news headline...\",\n",
    "        description=\"Headline:\",\n",
    "        layout=widgets.Layout(width=\"99%\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "interaction.widget.children[1].description = \"Classify!\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# reset the seed\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "# define and intialize the neural network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=400000, emb_dim=300, num_classes=4):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.conv1 = nn.Conv1d(emb_dim, 128, kernel_size=3)\n",
    "        self.max_pool1d = nn.MaxPool1d(5)\n",
    "        self.flatten1 = nn.Flatten()\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.fc1 = nn.Linear(896, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    # x is data that will be passed through the network\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = torch.transpose(x,1,2)\n",
    "        x = self.flatten1(self.max_pool1d(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "# define helper function to train our model\n",
    "\n",
    "def train(train_loader, embedding_matrix, num_classes=4, epochs=12, learning_rate=0.001):\n",
    "\n",
    "    # initialise model\n",
    "    model = Net(\n",
    "        vocab_size=embedding_matrix.shape[0],\n",
    "        emb_dim=embedding_matrix.shape[1],\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    model.embedding.weight = torch.nn.parameter.Parameter(torch.FloatTensor(embedding_matrix), False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "        for batch_idx, (X_train, y_train) in enumerate(train_loader, 1):\n",
    "            data, target = X_train.to(device), y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.binary_cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        print(f\"epoch: {epoch}, train_loss: {running_loss / n_batches:.6f}\")  # (Avg over batches)\n",
    "    return model, device\n",
    "\n",
    "# define helper function to test our model\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.binary_cross_entropy(output, target, reduction=\"sum\").item()\n",
    "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            target_index = target.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target_index).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Average loss over dataset samples\n",
    "    print(f\"val_loss: {test_loss:.4f}, val_acc: {correct/len(test_loader.dataset):.4f}\") \n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        X = torch.as_tensor(self.data[index]).long()\n",
    "        y = torch.as_tensor(self.labels[index])\n",
    "        return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "%%time\n",
    "# fit the model:\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "trainloader = DataLoader(Dataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "testloader = DataLoader(Dataset(X_test, y_test), batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Training model...\")\n",
    "model, device = train(\n",
    "    trainloader,\n",
    "    embedding_matrix,\n",
    "    num_classes=num_classes,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    ")\n",
    "print(\"Evaluating model...\")\n",
    "test(model, testloader, device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training model...\n",
      "epoch: 1, train_loss: 0.241407\n",
      "epoch: 2, train_loss: 0.204257\n",
      "epoch: 3, train_loss: 0.190396\n",
      "epoch: 4, train_loss: 0.180207\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d3130fdc8cd9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, embedding_matrix, num_classes, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mn_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;31m# Stores underlying RecordFunction as a tensor. TODO: move to custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;31m# class (https://github.com/pytorch/pytorch/issues/35026).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "8e93762ce26157543dddba731cb4c23b4c88be6f22dc743c1c6a9259770c1b2a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}